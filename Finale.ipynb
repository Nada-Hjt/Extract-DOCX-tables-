{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Finale.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G4p0PGDD9c24",
        "outputId": "34d8faaf-a241-40a3-dec6-396d91367cae"
      },
      "source": [
        "#!pip3 install python-docx\r\n",
        "#!pip install docx\r\n",
        "#!pip install spacy\r\n",
        "#!python -m spacy download fr_core_news_sm\r\n",
        "\r\n",
        "from docx import Document\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import nltk\r\n",
        "import re\r\n",
        "import string\r\n",
        "import fr_core_news_sm\r\n",
        "import numpy as np\r\n",
        "from nltk.corpus import stopwords\r\n",
        "nlp = fr_core_news_sm.load()\r\n",
        "nltk.download('stopwords')\r\n",
        "\r\n",
        "def cleaning(test):\r\n",
        "   test=test.lower()\r\n",
        "   test = test.translate(str.maketrans('','',string.punctuation))\r\n",
        "   test=test.strip()\r\n",
        "   test=re.sub('\\s+',' ',test)\r\n",
        "   test=''.join( c for c in test if  c not in \"«’»'–\" )\r\n",
        "   return test\r\n",
        "   \r\n",
        "def return_token(sentence):\r\n",
        "    doc = nlp(sentence)\r\n",
        "    return [X.text for X in doc]\r\n",
        "\r\n",
        "def remove_stopw(test):\r\n",
        "    stopWords = set(stopwords.words('french'))\r\n",
        "    clean_words = ''\r\n",
        "    for token in return_token(test):\r\n",
        "        if token not in stopWords:\r\n",
        "            clean_words = clean_words + token + ' '\r\n",
        "    return clean_words\r\n",
        "\r\n",
        "def lemm(test):\r\n",
        "  tokens = [] \r\n",
        "  test3 = nlp(test)\r\n",
        "  for token in test3: \r\n",
        "     tokens.append(token) \r\n",
        "  \r\n",
        "  lemmatized_sentence = \" \".join([token.lemma_ for token in test3]) \r\n",
        "  return lemmatized_sentence\r\n",
        "\r\n",
        "def normalize(test):\r\n",
        "  test=cleaning(test)\r\n",
        "  test=remove_stopw(test)\r\n",
        "  test=lemm(test)\r\n",
        "  return test\r\n",
        "\r\n",
        "document = Document('/content/input.docx')\r\n",
        "tables = []\r\n",
        "for index,table in enumerate(document.tables): #index : numero de table \r\n",
        "    df = [['' for i in range(len(table.columns))] for j in range(len(table.rows))]\r\n",
        "    for i, row in enumerate(table.rows):\r\n",
        "        for j, cell in enumerate(row.cells):       #chaque tableau on le crée un Dataframe\r\n",
        "            df[i][j] = normalize(cell.text)\r\n",
        "    if pd.DataFrame(df).iloc[0][0] == \"initial condition\" :\r\n",
        "      for E in df: del E[3]\r\n",
        "      pd.DataFrame(df).to_excel(\"Table#\"+str(index)+\".xlsx\") "
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}